## ADDED Requirements

### Requirement: Architecture Adapter Interface
The system SHALL provide an adapter interface to support multiple model architectures with a unified API for training and inference.

#### Scenario: BaseAdapter interface definition
- **WHEN** a new architecture adapter is created
- **THEN** adapter implements required methods:
  - `forward(input_ids, attention_mask=None) -> logits`: Forward pass returning logits
  - `get_config() -> dict`: Returns model configuration
  - `save_checkpoint(path) -> None`: Saves model state to disk
  - `load_checkpoint(path) -> None`: Loads model state from disk
  - `get_architecture_type() -> str`: Returns architecture identifier
  - `get_num_parameters() -> int`: Returns total parameter count
- **AND** adapter handles architecture-specific implementation details

#### Scenario: CustomTransformerAdapter wraps existing Transformer
- **WHEN** CustomTransformerAdapter is instantiated
- **THEN** wraps existing custom Transformer implementation
- **AND** implements BaseAdapter interface
- **AND** maintains backward compatibility with existing behavior
- **AND** architecture_type is "custom-transformer"

#### Scenario: QwenAdapter supports Qwen models
- **WHEN** QwenAdapter is instantiated with Qwen model
- **THEN** wraps HuggingFace Qwen model
- **AND** implements BaseAdapter interface
- **AND** handles Qwen-specific layer naming and structure
- **AND** integrates Qwen tokenizer
- **AND** architecture_type is "qwen"

#### Scenario: Adapter selection based on architecture type
- **WHEN** model is loaded with specified architecture_type
- **THEN** appropriate adapter is selected and instantiated
- **AND** adapter is initialized with model weights and config
- **AND** adapter is ready for training or inference

#### Scenario: Adapter handles tokenizer integration
- **WHEN** adapter is instantiated
- **THEN** adapter loads and stores appropriate tokenizer
- **AND** tokenizer is accessible via `get_tokenizer()` method
- **AND** tokenizer is used consistently for encoding/decoding

### Requirement: Multi-Architecture Model Loading
The system SHALL support loading models of different architectures from local storage using the adapter system.

#### Scenario: Load custom Transformer model
- **WHEN** model_id points to custom-transformer architecture
- **THEN** system loads model using CustomTransformerAdapter
- **AND** initializes with config parameters from TrainingConfig
- **AND** model is ready for training or inference

#### Scenario: Load Qwen model
- **WHEN** model_id points to Qwen architecture model
- **THEN** system loads model using QwenAdapter
- **AND** loads weights from model's local directory
- **AND** loads model's original config
- **AND** model is ready for training or inference

#### Scenario: Architecture-agnostic training loop
- **WHEN** training loop calls model.forward()
- **THEN** correct adapter's forward method is called
- **AND** adapter returns logits in consistent format
- **AND** training loop works identically regardless of architecture
- **AND** gradients flow correctly through adapter

#### Scenario: Model parameter access
- **WHEN** code requests model parameters (e.g., for optimizer)
- **THEN** adapter provides access to trainable parameters
- **AND** parameters are returned in format compatible with PyTorch optimizers
- **AND** adapter handles architecture-specific parameter organization

### Requirement: Qwen Architecture Support
The system SHALL support the Qwen family of models with proper weight loading, tokenizer integration, and training compatibility.

#### Scenario: Qwen weight loading from HuggingFace format
- **WHEN** QwenAdapter loads weights from imported model
- **THEN** converts HuggingFace state dict to internal format
- **AND** handles layer name mapping (e.g., "transformer.h" â†’ internal naming)
- **AND** validates all required layers are present
- **AND** reports any missing or unexpected layers

#### Scenario: Qwen tokenizer integration
- **WHEN** QwenAdapter is instantiated
- **THEN** loads Qwen tokenizer from model directory
- **AND** tokenizer is used for encoding input text
- **AND** tokenizer is used for decoding generated tokens
- **AND** special tokens are handled correctly

#### Scenario: Qwen model forward pass
- **WHEN** QwenAdapter.forward() is called with input_ids
- **THEN** passes input through Qwen model layers
- **AND** returns logits in shape [batch_size, seq_len, vocab_size]
- **AND** supports causal attention masking
- **AND** handles position embeddings correctly

#### Scenario: Qwen model fine-tuning
- **WHEN** QwenAdapter is used in training loop
- **THEN** gradients flow correctly through all layers
- **AND** parameters are updated by optimizer
- **AND** model learns and loss decreases
- **AND** fine-tuned model can be checkpointed

#### Scenario: Qwen model generation
- **WHEN** QwenAdapter is used for text generation
- **THEN** supports sampling with temperature
- **AND** supports greedy decoding
- **AND** handles end-of-sequence tokens correctly
- **AND** generated text is coherent

### Requirement: Architecture Extensibility
The system SHALL provide clear patterns and documentation for adding support for new model architectures beyond Qwen.

#### Scenario: Add new architecture adapter
- **WHEN** developer wants to add support for a new architecture (e.g., LLaMA)
- **THEN** documentation explains how to create new adapter class
- **AND** documentation lists required methods to implement
- **AND** documentation provides example adapter implementation
- **AND** adapter registration system allows easy integration

#### Scenario: Adapter validation
- **WHEN** new adapter is implemented
- **THEN** system provides validation utilities to test adapter compliance
- **AND** validates all required methods are implemented
- **AND** validates forward pass returns correct shape
- **AND** validates checkpoint save/load roundtrip works

