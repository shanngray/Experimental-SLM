# Large Model Configuration
# This configuration uses a larger model architecture for better quality results.
# Use this when you have sufficient computational resources and want the best
# possible model performance.
#
# Trade-offs:
# - Better model capacity and quality
# - Slower training and inference
# - Higher memory usage
# - Requires more training steps to converge

# Model Architecture - Larger dimensions
n_layers: 6  # More transformer blocks for increased capacity
d_model: 512  # Larger model dimension
n_heads: 8  # More attention heads (must divide d_model)
d_ff: 2048  # Larger feed-forward dimension
dropout: 0.1

# Dataset
train_ratio: 0.95
max_seq_len: 256

# Training Loop - May need more steps for larger model
max_steps: 20000  # More steps for larger model to converge
checkpoint_cadence: 1000

# Optimizer - Smaller batch size due to memory constraints
learning_rate: 3.0e-4
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
batch_size: 8  # Smaller batch size due to larger model memory usage

# Evaluation and Sampling
eval_cadence: null
sampling_cadence: null
sampling_temperature: 1.0
sampling_prompt: "The"
sampling_max_length: 100
sampling_seed: 42

# Other
seed: null
hooks: null

