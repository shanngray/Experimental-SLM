# Small Model Configuration
# This configuration uses a smaller model architecture for quick experimentation
# and faster training. Use this when you want to test ideas quickly or have
# limited computational resources.
#
# Trade-offs:
# - Faster training and inference
# - Lower memory usage
# - Less model capacity (may not capture complex patterns)

# Model Architecture - Smaller dimensions
n_layers: 2  # Fewer transformer blocks for faster training
d_model: 128  # Smaller model dimension
n_heads: 2  # Fewer attention heads (must divide d_model)
d_ff: 512  # Smaller feed-forward dimension
dropout: 0.1

# Dataset
train_ratio: 0.95
max_seq_len: 256

# Training Loop - Adjusted for smaller model
max_steps: 10000
checkpoint_cadence: 1000

# Optimizer - Can use larger batch size with smaller model
learning_rate: 3.0e-4
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
batch_size: 32  # Larger batch size possible with smaller model

# Evaluation and Sampling
eval_cadence: null
sampling_cadence: null
sampling_temperature: 1.0
sampling_prompt: "The"
sampling_max_length: 100
sampling_seed: 42

# Other
seed: null
hooks: null

