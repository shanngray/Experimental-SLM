# Qwen Base Model Configuration
# This config uses an imported Qwen model from HuggingFace.
# First import the model: python main.py import-model Qwen/Qwen-0.5B

# ============================================================================
# Model Selection
# ============================================================================
# Specify which model to use from the registry.
# This model must be imported first using: python main.py import-model <model-id>
model_name: "qwen-0.5b-base"  # Model name from registry (change to match your imported model)

# ============================================================================
# Dataset Hyperparameters
# ============================================================================
train_ratio: 0.95
max_seq_len: 256  # Note: Qwen models may have different max_seq_len limits

# ============================================================================
# Training Loop Hyperparameters
# ============================================================================
max_steps: 10000
checkpoint_cadence: 1000

# ============================================================================
# Optimizer Hyperparameters
# ============================================================================
learning_rate: 3.0e-4  # Lower learning rate recommended for fine-tuning pretrained models
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
batch_size: 16

# ============================================================================
# Evaluation and Sampling Hyperparameters
# ============================================================================
eval_cadence: null
sampling_cadence: null
sampling_temperature: 1.0
sampling_prompt: "The"
sampling_max_length: 100
sampling_seed: 42

# ============================================================================
# Other Hyperparameters
# ============================================================================
seed: null
hooks: null

# ============================================================================
# Quantization Hyperparameters
# ============================================================================
quantization_mode: null
quantization_bits: 8
quantization_type: "static"
enable_quantized_finetuning: false

# ============================================================================
# Notes
# ============================================================================
# - Architecture hyperparameters (n_layers, d_model, etc.) are loaded from the model,
#   not from this config file.
# - The model's native tokenizer will be used automatically.
# - To use a custom Transformer instead, set model_name: null and specify
#   architecture hyperparameters.

