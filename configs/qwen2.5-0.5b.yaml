# Qwen2.5-0.5B Model Configuration
# This config uses the imported Qwen2.5-0.5B model from HuggingFace.
# Model: https://huggingface.co/Qwen/Qwen2.5-0.5B
# 
# Model Properties:
# - Architecture: Transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias, tied word embeddings
# - Parameters: 0.49B (0.36B non-embedding)
# - Layers: 24
# - Attention: Grouped Query Attention (GQA) with 14 Q heads and 2 KV heads
# - Context Length: 32,768 tokens (full support)
# - Type: Base causal language model (pretrained, not instruction-tuned)

# ============================================================================
# Model Selection
# ============================================================================
# Specify which model to use from the registry.
# This model must be imported first using: python main.py import-model Qwen/Qwen2.5-0.5B
model_name: "qwen-qwen25-05b"  # Model name from registry

# ============================================================================
# Dataset Hyperparameters
# ============================================================================
train_ratio: 0.95
max_seq_len: 2048  # Qwen2.5-0.5B supports up to 32,768 tokens, but 2048 is more practical for training
                   # Increase this if you have sufficient GPU memory

# ============================================================================
# Training Loop Hyperparameters
# ============================================================================
max_steps: 10000
checkpoint_cadence: 1000

# ============================================================================
# Optimizer Hyperparameters
# ============================================================================
# Lower learning rate recommended for fine-tuning pretrained models
learning_rate: 1.0e-4  # Conservative learning rate for fine-tuning
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
batch_size: 8  # Smaller batch size due to model size and longer sequences
               # Adjust based on available GPU memory

# ============================================================================
# Evaluation and Sampling Hyperparameters
# ============================================================================
eval_cadence: 500  # Evaluate every 500 steps to monitor progress
sampling_cadence: 500  # Generate samples every 500 steps
sampling_temperature: 1.0
sampling_prompt: "The"
sampling_max_length: 100
sampling_seed: 42

# ============================================================================
# Other Hyperparameters
# ============================================================================
seed: 42  # Set seed for reproducibility
hooks: null

# ============================================================================
# Quantization Hyperparameters
# ============================================================================
quantization_mode: null
quantization_bits: 8
quantization_type: "static"
enable_quantized_finetuning: false

# ============================================================================
# Notes
# ============================================================================
# - Architecture hyperparameters (n_layers, d_model, etc.) are loaded from the model,
#   not from this config file. The model has 24 layers with GQA (14 Q heads, 2 KV heads).
# - The model's native tokenizer will be used automatically.
# - This is a base model (pretrained), not instruction-tuned. Consider applying
#   post-training techniques (SFT, RLHF, continued pretraining) for better results.
# - The model supports up to 32,768 tokens context length, but training with
#   max_seq_len > 2048 will require significant GPU memory.
# - Adjust batch_size based on your GPU memory. With max_seq_len=2048, you may
#   need to reduce batch_size further if you encounter OOM errors.

